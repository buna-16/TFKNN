# -*- coding: utf-8 -*-
"""project_review_1.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1X8yY-CEkjnYF7DFf7vtcGc9L8GFAaHqQ

#**IMPORTING THE PACKAGES**
"""

#Connecting to the Google Drive
from google.colab import drive
drive.mount('/content/drive')

# Commented out IPython magic to ensure Python compatibility.
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import accuracy_score, classification_report,confusion_matrix
from mlxtend.plotting import plot_decision_regions
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
sns.set()
import warnings
warnings.filterwarnings('ignore')
# %matplotlib inline

# Load your dataset (replace 'diabetes_dataset.csv' with your data)
data = pd.read_csv('/content/drive/MyDrive/diabetes.csv')

#printing the first five values
data.head()

#printing the last five values
data.tail()

#printing the rows and columns
data.shape

# describing the details of the table
data.describe()

# transposing the values
data.describe().T

#copying the data to a new variable
diabetes_data_copy = data.copy(deep = True)
diabetes_data_copy[['Glucose','BloodPressure','SkinThickness','Insulin','BMI']] = diabetes_data_copy[['Glucose','BloodPressure','SkinThickness','Insulin','BMI']].replace(0,np.NaN)
## showing the count of Nans
print(diabetes_data_copy.isnull().sum())

"""# **HISTOGRAM PLOT FOR UNCLEANED DATA**

"""

p = data.hist(figsize = (10,10))

"""#**Replacing Missing values with mean values**"""

#Taking mean for the null values
diabetes_data_copy['Glucose'].fillna(diabetes_data_copy['Glucose'].mean(), inplace = True)
diabetes_data_copy['BloodPressure'].fillna(diabetes_data_copy['BloodPressure'].mean(), inplace = True)
diabetes_data_copy['SkinThickness'].fillna(diabetes_data_copy['SkinThickness'].median(), inplace = True)
diabetes_data_copy['Insulin'].fillna(diabetes_data_copy['Insulin'].median(), inplace = True)
diabetes_data_copy['BMI'].fillna(diabetes_data_copy['BMI'].median(), inplace = True)

"""#**HISTOGRAM PLOT FOR CLEANED DATA**"""

p = diabetes_data_copy.hist(figsize = (10,10))

"""# **HEAT MAP FOR UNCLEAN DATA**"""

plt.figure(figsize=(12,10))  # on this line I just set the size of figure to 12 by 10.
p=sns.heatmap(data.corr(), annot=True,cmap ='RdYlGn')  # seaborn has very simple solution for heatmap

"""# **HEAT MAP FOR CLEAN DATA**"""

plt.figure(figsize=(12,10))  # on this line I just set the size of figure to 12 by 10.
p=sns.heatmap(diabetes_data_copy.corr(), annot=True,cmap ='RdYlGn')  # seaborn has very simple solution for heatmap

diabetes_data_copy.head()

"""#**TRAIN TEST SPLIT**"""

X = diabetes_data_copy.drop(['Outcome'], axis=1)
Y = diabetes_data_copy.Outcome
x_train, x_test, y_train, y_test = train_test_split(X, Y, test_size=0.2)

print(X.shape)

"""## **SMOTE DATA**"""

from imblearn.over_sampling import SMOTE
smote = SMOTE()

X_smt, Y_smt = smote.fit_resample(X, Y)

X_train, X_test, Y_train, Y_test = train_test_split(X_smt, Y_smt, test_size=0.2)

# smote_x_train, smote_y_train  = smote.fit_resample(X_train, Y_train)

print(X_smt.shape)

"""#**K FOLD VALIDATION**


"""

from sklearn.model_selection import KFold

fold = KFold(n_splits=10, random_state=42, shuffle=True)

"""# For **KNN**"""

x_array=X.to_numpy() #converting data frame to array

sxc = StandardScaler()

accs = []
k=26
for train_index, test_index in fold.split(x_array,Y):
  x_train, x_test, y_train, y_test = x_array[train_index], x_array[test_index], Y[train_index], Y[test_index]
  sxc.fit(x_train)
  x_train = sxc.transform(x_train)
  x_test = sxc.transform(x_test)
  knn = KNeighborsClassifier(n_neighbors=k)
  knn.fit(x_train, y_train)
  predicted_on_this_fold = knn.predict(x_test)
  accuracy = accuracy_score(y_test, predicted_on_this_fold)
  print("Accuracy on this fold => ", accuracy*100)
  accs.append(accuracy * 100)
  np.random.seed(238)

print("Maximum Accuracy => ", max(accs))
print("Average Accuracy => ", np.average(accs))

"""#**CONFUSION MATRIX FOR KNN**"""

conf_mat = confusion_matrix(y_test, predicted_on_this_fold)
print(conf_mat)

print(pd.crosstab(y_test, predicted_on_this_fold))

print(classification_report(y_test, predicted_on_this_fold)) #the number of occurrences of each particular class in the true responses

"""#**Naive Bayes**"""

from sklearn.naive_bayes import GaussianNB

NB_model = GaussianNB(var_smoothing=0.01)

NB_model.fit(x_train, y_train)

NB_pred = NB_model.predict(x_test)

np.random.seed(238)
print(accuracy_score(y_test, NB_pred)*100)



"""#**Decision Tree**"""

from sklearn.tree import DecisionTreeClassifier

DT_model = DecisionTreeClassifier(criterion='gini')



DT_model.fit(x_train,y_train)

y_pred = DT_model.predict(x_test)

print(accuracy_score(y_test, y_pred)*100)

"""#**FUZZY KNN**"""

import numpy as np
from sklearn.base import BaseEstimator, ClassifierMixin

class FuzzyKNNClassifier(BaseEstimator, ClassifierMixin):
    def __init__(self, k):
        self.k = k

    def calculate_membership(self, x, data):
        distances = np.linalg.norm(data - x, axis=1)
        memberships = 1 / (distances + 1e-6)
        return memberships

    def fit(self, X, y):
        self.X_train = X
        self.y_train = y

    def predict(self, X):
        y_pred = []

        for x in X.values:
            memberships = self.calculate_membership(x, self.X_train)
            nearest_indices = np.argsort(memberships)[-self.k:]
            nearest_labels = self.y_train.iloc[nearest_indices]

            # Predict the class with the highest membership
            prediction = nearest_labels.mode().iloc[0]  # Use mode for fuzzy predictions
            y_pred.append(prediction)

        self.predictions = np.array(y_pred)  # Store predictions for debugging
        return self.predictions

    def score(self, X, y):
        y_pred = self.predict(X)
        accuracy = accuracy_score(y,y_pred)
        return accuracy

k=5
fuz_accs = []
x_array=X_smt.to_numpy()
for train_index, test_index in fold.split(x_array,Y_smt):
  x_train, x_test, y_train, y_test = x_array[train_index], x_array[test_index], Y_smt[train_index], Y_smt[test_index]
  sxc.fit(x_train)
  x_train = sxc.transform(x_train)
  x_test = sxc.transform(x_test)
  x_test = pd.DataFrame(x_test)
  fuzzy_knn = FuzzyKNNClassifier(k)
  fuzzy_knn.fit(x_train, y_train)
  predicted_on_this_fold = fuzzy_knn.predict(x_test)
  accuracy = accuracy_score(y_test, predicted_on_this_fold)
  np.random.seed(238)

  print("Accuracy on this fold => ", accuracy*100)
  fuz_accs.append(accuracy * 100)

print("Maximum Accuracy => ", max(fuz_accs))
print("Average Accuracy => ", np.average(fuz_accs))

conf_mat = confusion_matrix(y_test, predicted_on_this_fold)
print(conf_mat)

print(pd.crosstab(y_test, predicted_on_this_fold))

print(classification_report(y_test, predicted_on_this_fold))



"""#**Grid Search for KNN** Experimental 1.0


"""

from sklearn.model_selection import GridSearchCV


# Define the parameter grid
param_grid = {
    'n_neighbors': [3, 10, 23, 15, 27, 29, 35, 43, 75, 53],
}

# Perform GridSearch using the KNN
grid_search = GridSearchCV(knn, param_grid, cv=10, return_train_score=False)
grid_search.fit(x_train, y_train)

grid_search.cv_results_

results_df = pd.DataFrame(grid_search.cv_results_)


# Print the best hyperparameters and corresponding score
print("Best Hyperparameters: ", grid_search.best_params_)
print("Best Score: ", grid_search.best_score_)
print(results_df)

gd_knn_pred = grid_search.predict(x_test)

print("Accuracy of GridSearch KNN => ", accuracy_score(y_test, gd_knn_pred)*100)

print(pd.crosstab(y_test, gd_knn_pred))

print(classification_report(y_test, gd_knn_pred))



"""#**Grid Search For Naive Bayes**"""

from sklearn.model_selection import GridSearchCV


# Define the parameter grid
param_grid = {
    'var_smoothing': [0.01, 0.02, 0.03],
}

# Perform GridSearch using the NB
grid_search = GridSearchCV(NB_model, param_grid, cv=10, return_train_score=False)
grid_search.fit(x_train, y_train)

grid_search.cv_results_

results_df = pd.DataFrame(grid_search.cv_results_)


# Print the best hyperparameters and corresponding score
print("Best Hyperparameters: ", grid_search.best_params_)
print("Best Score: ", grid_search.best_score_)
print(results_df)

NB_pred = grid_search.predict(x_test)

print(accuracy_score(y_test, NB_pred)*100)
print(pd.crosstab(y_test,NB_pred))

print(classification_report(y_test, NB_pred))



"""#**Grid Search For Decision Tree**"""

from sklearn.model_selection import GridSearchCV


# Define the parameter grid
param_grid = {
    'random_state':[1,10,42,50,23],
}

# Perform GridSearch using the DT
grid_search = GridSearchCV(DT_model, param_grid, cv=10, return_train_score=False)
grid_search.fit(x_train, y_train)

grid_search.cv_results_

results_df = pd.DataFrame(grid_search.cv_results_)


# Print the best hyperparameters and corresponding score
print("Best Hyperparameters: ", grid_search.best_params_)
print("Best Score: ", grid_search.best_score_)
print(results_df)

DT_pred = grid_search.predict(x_test)

print("Accuracy of GridSearch DecisionTree Model => ", accuracy_score(y_test, DT_pred)*100)
print(pd.crosstab(y_test, DT_pred))

print(classification_report(y_test, DT_pred))

"""#**Grid Search for Fuzzy KNN** (TFKNN)"""

from sklearn.model_selection import GridSearchCV


# Define the parameter grid
param_grid = {
    'k': [*range(1,101)],
}

# Create an instance of FuzzyKNNClassifier
fuzzy_knn = FuzzyKNNClassifier(5)

# Perform GridSearch using the custom FuzzyKNNClassifier
grid_search = GridSearchCV(fuzzy_knn, param_grid, cv=10, return_train_score=False)
grid_search.fit(X_train, Y_train)

grid_search.cv_results_

results_df = pd.DataFrame(grid_search.cv_results_)


# Print the best hyperparameters and corresponding score
print("Best Hyperparameters: ", grid_search.best_params_)
print("Best Score: ", grid_search.best_score_)
print(results_df)

TFKNN_pred = grid_search.predict(X_test)

print("Accuracy of GridSearch Fuzzy KNN => ", accuracy_score(Y_test, TFKNN_pred)*100)
print(pd.crosstab(Y_test, TFKNN_pred))

print(classification_report(Y_test, TFKNN_pred))



