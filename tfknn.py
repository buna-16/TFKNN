# -*- coding: utf-8 -*-
"""tfknn

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1eIcCrU64vJGrRnZf62wzJT_MLI38FWFU

# Installing packages

**Installing fuzzy c means**
"""

!pip install fuzzy-c-means

"""**Installing fuzzy**"""

!pip install -U scikit-fuzzy

"""# **Import Libraries**"""

import pandas as pd
import io
import numpy as np
from scipy import stats
import seaborn as sns
import matplotlib.pyplot as plt
import skfuzzy as fuzz
from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay
from sklearn.model_selection import train_test_split
from sklearn.svm import SVC
from sklearn.metrics import classification_report

"""*Mount drive*"""

from google.colab import drive
drive.mount('/content/drive')

"""*copy path*"""

path = "/content/drive/MyDrive/miniproj/diabetes.csv"
df_bonus = pd.read_csv(path)

"""# Loading Dataset"""

df = pd.read_csv("/content/drive/MyDrive/miniproj/diabetes.csv")
print(df)

df.head()

df.describe()

"""# Outlier Rejection"""

import pandas as pd
import numpy as np

# Load the Pima Diabetes dataset (replace 'pima_diabetes.csv' with the actual path to your dataset)
data = pd.read_csv('/content/drive/MyDrive/miniproj/diabetes.csv')

# Define a function to reject outliers using IQR
def reject_outliers_iqr(data, threshold=1.5):
    Q1 = data.quantile(0.25)
    Q3 = data.quantile(0.75)
    IQR = Q3 - Q1

    lower_bound = Q1 - threshold * IQR
    upper_bound = Q3 + threshold * IQR

    # Create a mask to filter out outliers
    outlier_mask = ((data < lower_bound) | (data > upper_bound)).any(axis=1)

    # Return the dataset without outliers
    return data[~outlier_mask]

# Select the numerical columns from the dataset (exclude any non-numeric columns)
numeric_data = data.select_dtypes(include=[np.number])

# Perform outlier rejection using IQR for each numeric column
filtered_data = reject_outliers_iqr(numeric_data)

# Display the filtered dataset
print(filtered_data)

"""# Imputing Missing Values"""

import pandas as pd
import numpy as np
from sklearn.impute import SimpleImputer

# Load the Pima Diabetes dataset (replace 'pima_diabetes.csv' with the actual path to your dataset)
data = pd.read_csv('/content/drive/MyDrive/miniproj/diabetes.csv')

# Check for missing values
missing_values = data.isnull().sum()

# Select columns with missing values (if any)
columns_with_missing = missing_values[missing_values > 0].index.tolist()

# Apply imputation to columns with missing values
for column in columns_with_missing:
    if data[column].dtype == 'object':
        # For categorical data, impute with the most frequent value (mode)
        imputer = SimpleImputer(strategy='most_frequent')
    else:
        # For numeric data, impute with the mean
        imputer = SimpleImputer(strategy='mean')

    # Fit the imputer on the column and transform it
    data[column] = imputer.fit_transform(data[[column]])

# Save the imputed dataset (if needed)
data.to_csv('imputed_pima_diabetes.csv', index=False)

"""# Z score"""

import pandas as pd
from sklearn.preprocessing import StandardScaler

# Create a copy of the dataset
data = pd.read_csv('/content/drive/MyDrive/miniproj/diabetes.csv')

# Select numerical features for normalization
numerical_features = ['Pregnancies', 'Glucose', 'BloodPressure', 'SkinThickness', 'Insulin', 'BMI', 'DiabetesPedigreeFunction', 'Age']

# Create a subset of the copied DataFrame containing only the numerical features
numerical_data_copy = data[numerical_features]

# Initialize the StandardScaler
scaler = StandardScaler()

# Fit the scaler to your numerical data copy and transform it
normalized_data_copy = scaler.fit_transform(numerical_data_copy)

# Create a DataFrame with the normalized values
normalized_df_copy = pd.DataFrame(normalized_data_copy, columns=numerical_features)

# Combine the normalized numerical features with the non-numerical features (if needed)
normalized_df_copy = pd.concat([normalized_df_copy, data[['Outcome']]], axis=1)

# Display the first few rows of the normalized DataFrame copy
print(normalized_df_copy.head())

"""# Standardization"""

import pandas as pd
from sklearn.preprocessing import StandardScaler

# Load the imputed Pima Indians Diabetes dataset (replace 'imputed_pima_diabetes.csv' with the actual path to your dataset)
data = pd.read_csv('imputed_pima_diabetes.csv')

# Extract the features (X) and the target variable (y)
X = data.drop('Outcome', axis=1)  # Assuming 'Outcome' is the target variable
y = data['Outcome']

# Initialize the StandardScaler
scaler = StandardScaler()

# Fit the scaler on the features and transform them
X_scaled = scaler.fit_transform(X)

# Create a DataFrame from the scaled features
scaled_data = pd.DataFrame(X_scaled, columns=X.columns)

# Combine the scaled features with the target variable
scaled_data['Outcome'] = y

# Display the standardized dataset
print(scaled_data)

"""# Correlation matrix"""

# Create a copy of the dataset
data_corr = pd.read_csv('imputed_pima_diabetes.csv')

# Calculate the correlation matrix
correlation_matrix = data_corr.corr()

# Display the correlation matrix
print(correlation_matrix)


# Calculate the correlation matrix
correlation_matrix = data_corr.corr()

# Plot the correlation matrix as a heatmap
plt.figure(figsize=(10, 8))
sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', linewidths=0.5)
plt.title("Correlation Matrix for Pima Indians Diabetes Dataset")
plt.show()

"""# Train and Test"""

X = data.drop(['Outcome'], axis=1)
Y = data.Outcome
X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.2, stratify=Y)

"""# K fold"""

from sklearn.model_selection import KFold
fold = KFold(n_splits=10, random_state=10, shuffle=True)

"""# KNN Classifier"""

#Create a k-NN classifier with 7 neighbors
knn = KNeighborsClassifier(n_neighbors = 15)

#Fit the classifier to the training data
knn.fit(X_train, y_train)

#Print the accuracy
print(knn.score(X_test, y_test))
print("Accuracy = {}".format(round(knn.score(X_test, y_test),2) * 100)+"%")

#Setup arrays to store train and test accuracies
neighbors = np.arange(1, 9)
train_accuracy = np.empty(len(neighbors))
test_accuracy = np.empty(len(neighbors))
#Loop over different values of k
for i, k in enumerate(neighbors):

    # Setup a k-NN Classifier with k neighbors
    knn = KNeighborsClassifier(n_neighbors=k)

    #Fit the classifier to the training data
    knn.fit(X_train, y_train)

    #Compute accuracy on the training set
    train_accuracy[i] = knn.score(X_train, y_train)

    #Compute accuracy on the test set
    test_accuracy[i] = knn.score(X_test, y_test)

    # Predict the labels of the test data: y_pred
y_pred = knn.predict(X_test)

# Generate the confusion matrix and classification report
print(confusion_matrix(y_test, y_pred))
print(classification_report(y_test, y_pred))

"""# fuzzy knn"""

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
import numpy as np
import skfuzzy as fuzz

from sklearn.base import BaseEstimator, ClassifierMixin
from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import accuracy_score
from collections import defaultdict

class FuzzyKNN(BaseEstimator, ClassifierMixin):
    def __init__(self, k=3, plot=False):
        self.k = k
        self.plot = plot


    def fit(self, X, y=None):
        self._check_params(X,y)
        self.X = X
        self.y = y

        self.xdim = len(self.X[0])
        self.n = len(y)

        classes = list(set(y))
        classes.sort()
        self.classes = classes

        self.df = pd.DataFrame(self.X)
        self.df['y'] = self.y

        self.memberships = self._compute_memberships()

        self.df['membership'] = self.memberships

        self.fitted_ = True
        return self


    def predict(self, X):
        if self.fitted_ == None:
            raise Exception('predict() called before fit()')
        else:
            m = 2
            y_pred = []

            for x in X:
                neighbors = self._find_k_nearest_neighbors(pd.DataFrame.copy(self.df), x)

                votes = {}
                for c in self.classes:
                    den = 0
                    for n in range(self.k):
                        dist = np.linalg.norm(x - neighbors.iloc[n,0:self.xdim])
                        den += 1 / (dist ** (2 / (m-1)))

                    neighbors_votes = []
                    for n in range(self.k):
                        dist = np.linalg.norm(x - neighbors.iloc[n,0:self.xdim])
                        num = (neighbors.iloc[n].membership[c]) / (dist ** (2 / (m-1)))

                        vote = num/den
                        neighbors_votes.append(vote)
                    votes[c] = np.sum(neighbors_votes)

                pred = max(votes.items(), key=operator.itemgetter(1))[0]
                y_pred.append((pred, votes))

            return y_pred


    def score(self, X, y):
        if self.fitted_ == None:
            raise Exception('score() called before fit()')
        else:
            predictions = self.predict(X)
            y_pred = [t[0] for t in predictions]
            confidences = [t[1] for t in predictions]

            return accuracy_score(y_pred=y_pred, y_true=y)


    def _find_k_nearest_neighbors(self, df, x):
        X = df.iloc[:,0:self.xdim].values

        df['distances'] = [np.linalg.norm(X[i] - x) for i in range(self.n)]

        df.sort_values(by='distances', ascending=True, inplace=True)
        neighbors = df.iloc[0:self.k]

        return neighbors


    def _get_counts(self, neighbors):
        groups = neighbors.groupby('y')
        counts = {group[1]['y'].iloc[0]:group[1].count()[0] for group in groups}

        return counts


    def _compute_memberships(self):
        memberships = []
        for i in range(self.n):
            x = self.X[i]
            y = self.y[i]

            neighbors = self._find_k_nearest_neighbors(pd.DataFrame.copy(self.df), x)
            counts = self._get_counts(neighbors)

            membership = dict()
            for c in self.classes:
                try:
                    uci = 0.49 * (counts[c] / self.k)
                    if c == y:
                        uci += 0.51
                    membership[c] = uci
                except:
                    membership[c] = 0

            memberships.append(membership)
        return memberships


    def _check_params(self, X, y):
        if type(self.k) != int:
            raise Exception('"k" should have type int')
        elif self.k >= len(y):
            raise Exception('"k" should be less than no of feature sets')
        elif self.k % 2 == 0:
            raise Exception('"k" should be odd')

        if type(self.plot) != bool:
            raise Exception('"plot" should have type bool')

# Specify the hyperparameters (e.g., k, plot)
k_value = 3  # Replace with your desired value
plot_option = False  # Set to True if you want to visualize results

# Create an instance of the FuzzyKNN classifier with specified hyperparameters
fuzzy_knn = FuzzyKNN(k=k_value, plot=plot_option)

# Assuming you have training data (X_train, y_train)
fuzzy_knn.fit(X_train, y_train)

"""# Grid Search"""

# Choose the number of clusters (c)
c = 9

# Fuzzify the data
cntr, u, u0, d, jm, p, fpc = fuzz.cluster.cmeans(X_train.T, c, m=2, error=0.005, maxiter=1000)

# Assign cluster labels to the training data
cluster_labels = np.argmax(u, axis=0)

from sklearn.model_selection import StratifiedKFold
k=3
fuz_accs = []
for train_index, test_index in fold.split(x_array,Y):
  x_train, x_test, y_train, y_test = x_array[train_index], x_array[test_index], Y[train_index], Y[test_index]
  sxc.fit(x_train)
  fuzzy_knn = FuzzyKNN(k)
  x_train = sxc.transform(x_train)
  x_test = sxc.transform(x_test)
  x_test = pd.DataFrame(x_test)
  fuzzy_knn.fit(x_train, y_train)
  predicted_on_this_fold = fuzzy_knn.predict(x_test)
  accuracy = accuracy_score(y_test, predicted_on_this_fold)
  np.random.seed(338)

  print("Accuracy on this fold => ", accuracy*100)
  fuz_accs.append(accuracy * 100)

print("Final Accuracy => ", max(fuz_accs))

